from util import sentence
import natural_logic_model as nlm
import os
import random
import json

def process_data(train_ratio):
    #split the different parts of speech into train, validation, and test
    #determiners are not split
    train = dict()
    val = dict()
    test = dict()
    categories = ["agents", "transitive_verbs", "things", "determiners", "adverbs", "subject_adjectives","object_adjectives"]
    for c in categories:
        with open(os.path.join("data", c + ".txt"),"r") as f:
            stuff = f.readlines()
            if c != "transitive_verbs":
                stuff = [_.strip() for _ in stuff]
            else:
                stuff = [_.strip().split() for _ in stuff]
        random.shuffle(stuff)
        if c != "determiners":
            train[c] = stuff[:int(len(stuff)*train_ratio)]
            val[c] = stuff[int(len(stuff)*train_ratio):int(len(stuff)*(train_ratio+(1-train_ratio)*0.5))]
            test[c] = stuff[int(len(stuff)*(train_ratio+(1-train_ratio)*0.5)):]
        else:
            train[c] = stuff
            val[c] = stuff
            test[c] = stuff
    return train, val, test

def save_data(examples, name):
    #saves data in the SNLI format
    data = []
    for example in examples:
        example_dict = dict()
        example_dict["sentence1"] = example[0]
        example_dict["sentence2"] = example[2]
        example_dict["gold_label"] = example[1]
        example_dict["example_data"] = example[3]
        data.append(json.dumps(example_dict))
    with open(name, 'w') as f:
        for datum in data:
            f.write(datum + "\n")

def restricted(restrictions, enc):
    #This function determines whether an encoding of an NLI input
    #is restricted according to restrictions
    for i in range(len(enc)):
        if restrictions[i] < enc[i]:
            return True
    return False

def split_dict(filename, restrictions):
    #This function takes in a dictionary generated by build_simple_file or
    # build_boolean_file and divides the encoded NLI input keys by the label
    #they are mapped to
    with open(filename, 'r') as f:
        solutions= json.loads(f.read())
    e = dict()
    c = dict()
    p = dict()
    for i in solutions:
        if restricted(restrictions,json.loads(i)):
            continue
        if solutions[i] == "entails":
            e[i] = solutions[i]
        if solutions[i] == "contradicts":
            c[i] = solutions[i]
        if solutions[i] == "permits":
            p[i] = solutions[i]
    return e, c, p

def compute_relation(lexicon, relation_index):
    #This function takes in a lexicon list of words and a relation_index
    #and outputs the same random word twice if relation_index is 0,
    #the empty string and a random word if relation_index is 1,
    #the a random word and the empty string if relation_index is 2,
    #and two different random words if relation index is 3
    if relation_index == 0:
        premise_word = random.choice(lexicon + [""])
        hypothesis_word = premise_word
    if relation_index == 1:
        premise_word = ""
        hypothesis_word = random.choice(lexicon)
    if relation_index == 2:
        premise_word = random.choice(lexicon)
        hypothesis_word = ""
    if relation_index == 3:
        premise_word = random.choice(lexicon)
        hypothesis_word = select_new(lexicon, premise_word)
    return premise_word, hypothesis_word

def select_new(lexicon, old):
    #returns an element of lexicon that is not old without changing lexicon
    index = lexicon.index(old)
    lexicon.remove(old)
    new = random.choice(lexicon)
    lexicon.insert(index, old)
    return new

def encoding_to_independent_example(data, encoding, premise, hypothesis):
    new_premise, new_hypothesis = encoding_to_example(data,encoding)
    while nlm.compute_simple_relation(premise, new_premise) != "independence" or nlm.compute_simple_relation(premise, new_hypothesis) != "independence" or nlm.compute_simple_relation(hypothesis, new_hypothesis) != "independence" or nlm.compute_simple_relation(hypothesis, new_premise) != "independence":
        new_premise, new_hypothesis = encoding_to_example(data,encoding)
    return new_premise, new_hypothesis


def encoding_to_example(data, encoding):
    #takes in an encoding produced by build_simple_file
    #and outputs two sentence objects corresponding to the encoding
    dets = ["every", "not every", "some", "no"]
    psubject_noun = random.choice(data["agents"])
    pverb = random.choice(data["transitive_verbs"])
    pobject_noun = random.choice(data["things"])
    hsubject_noun = psubject_noun
    hverb = pverb
    hobject_noun = pobject_noun
    if encoding[-3] == 0:
        hsubject_noun = select_new(data["agents"], psubject_noun)
    if encoding[-2] == 0:
        hverb = select_new(data["transitive_verbs"], pverb)
    if encoding[-1] == 0:
        hobject_noun = select_new(data["things"], pobject_noun)
    padverb, hadverb  = compute_relation(data["adverbs"], encoding[-4])
    pobject_adjective, hobject_adjective = compute_relation(data["object_adjectives"], encoding[-5])
    psubject_adjective, hsubject_adjective = compute_relation(data["subject_adjectives"], encoding[-6])
    return sentence(psubject_noun, pverb, pobject_noun, encoding[0], padverb, psubject_adjective, pobject_adjective, dets[encoding[1]],dets[encoding[2]]), sentence(hsubject_noun, hverb, hobject_noun, encoding[3], hadverb, hsubject_adjective, hobject_adjective, dets[encoding[4]],dets[encoding[5]])

def generate_balanced_boolean_data(boolkeys, ekeys, ckeys, pkeys, size, data):
    #using encoded compound examples in boolkeys, and the encoded simple
    #examples in ekeys, ckeys, and pkeys, this function outputs a list of length
    #size with compound sentence examples
    result = []
    for i in range(size):
        encoding = json.loads(random.choice(boolkeys))
        if encoding[2] == 0:
            simple1_encoding = json.loads(random.choice(ekeys))
            premise1, hypothesis1 = encoding_to_example(data, simple1_encoding)
        if encoding[2] == 1:
            simple1_encoding = json.loads(random.choice(ckeys))
            premise1, hypothesis1 = encoding_to_example(data, simple1_encoding)
        if encoding[2] == 2:
            simple1_encoding = json.loads(random.choice(pkeys))
            premise1, hypothesis1 = encoding_to_example(data,simple1_encoding)
        if encoding[3] == 0:
            simple2_encoding = json.loads(random.choice(ekeys))
            premise2, hypothesis2 = encoding_to_independent_example(data, simple2_encoding, premise1, hypothesis1)
        if encoding[3] == 1:
            simple2_encoding = json.loads(random.choice(ckeys))
            premise2, hypothesis2 = encoding_to_independent_example(data, simple2_encoding, premise1, hypothesis1)
        if encoding[3] == 2:
            simple2_encoding = json.loads(random.choice(pkeys))
            premise2, hypothesis2 = encoding_to_independent_example(data, simple2_encoding, premise1, hypothesis1)
        conjunctions = ["or", "and", "then"]
        premise_conjunction = conjunctions[encoding[0]]
        hypothesis_conjunction = conjunctions[encoding[1]]
        premise_compound = premise1.string + " " + premise_conjunction + " " + premise2.string
        hypothesis_compound = hypothesis1.string+ " " + hypothesis_conjunction+ " " + hypothesis2.string
        if premise_conjunction == "then":
            premise_compound = "if " + premise_compound
        if hypothesis_conjunction == "then":
            hypothesis_compound = "if " + hypothesis_compound
        result.append((premise_compound, "entails", hypothesis_compound, [simple1_encoding, simple2_encoding, encoding]))
    return result

def trim_simple_encodings(data,ekeys, ckeys, pkeys):
    #This function trims simple nli encodings for use in
    #generating compound sentences see paper for why this is necessary
    new_ekeys = []
    new_ckeys = []
    new_pkeys = []
    for encoding in ekeys:
        premise, hypothesis = encoding_to_example(data,json.loads(encoding))
        if nlm.compute_simple_relation(premise, hypothesis) == "entails":
            new_ekeys.append(encoding)
    for encoding in ckeys:
        premise, hypothesis = encoding_to_example(data,json.loads(encoding))
        if nlm.compute_simple_relation(premise, hypothesis) == "alternation":
            new_ckeys.append(encoding)
    for encoding in pkeys:
        premise, hypothesis = encoding_to_example(data,json.loads(encoding))
        if nlm.compute_simple_relation(premise, hypothesis) == "independence":
            new_pkeys.append(encoding)
    return new_ekeys, new_ckeys, new_pkeys

def generate_balanced_data(simple_filename, boolean_filename, simple_size, boolean_size, data, restrictions=[1000000]*19):
    #Using simple_filename generated from build_simple_file and
    #boolean_filename from build_boolean_file generates a list of NLI inpus
    #with simple_size simple examples and boolean_size compound examples
    #restrictions can be used to restrict the types of examples generated
    e,c,p = split_dict(simple_filename, restrictions)
    ekeys = list(e.keys())
    ckeys = list(c.keys())
    pkeys = list(p.keys())
    label_size = int(simple_size/3)
    examples = []
    for i in range(label_size):
        encoding = json.loads(random.choice(ekeys))
        premise, hypothesis = encoding_to_example(data,encoding)
        examples.append((premise.string, "entails", hypothesis.string, [encoding]))
    for i in range(label_size):
        encoding = json.loads(random.choice(ckeys))
        premise, hypothesis = encoding_to_example(data,encoding)
        examples.append((premise.string, "contradicts", hypothesis.string, [encoding]))
    for i in range(label_size):
        encoding = json.loads(random.choice(pkeys))
        premise, hypothesis = encoding_to_example(data,encoding)
        examples.append((premise.string, "permits", hypothesis.string, [encoding]))
    bool_label_size = int(boolean_size/3)
    bool_e,bool_c,bool_p = split_dict(boolean_filename, [100000]*19)
    bool_ekeys = list(bool_e.keys())
    bool_ckeys = list(bool_c.keys())
    bool_pkeys = list(bool_p.keys())
    ekeys, ckeys, pkeys = trim_simple_encodings(data, ekeys, ckeys, pkeys)
    examples += generate_balanced_boolean_data(bool_ekeys,ekeys, ckeys, pkeys, bool_label_size, data)
    examples += generate_balanced_boolean_data(bool_ckeys, ekeys, ckeys, pkeys, bool_label_size, data)
    examples += generate_balanced_boolean_data(bool_pkeys, ekeys, ckeys, pkeys, bool_label_size, data)
    random.shuffle(examples)
    return examples
